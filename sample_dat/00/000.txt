現代において日常的に使用する基本的な語彙がどれぐらいのサイズであるかとい
うことは言語情報処理にとって重要な問題である．小学生用国語辞典の見出し語
数は約3万語，一般の国語辞典は約5万語，広辞苑クラスで約20万語である．一方，
言語処理に利用されるリソースでは，国立国語研分類語彙表が約10万語，NTT日本語
語彙大系やEDR辞書が約30万語である．

音声認識の場合には認識率の問題からできるだけ語彙を限定するということが行
われてきたが，言語処理の場合には基本的には「辞書の語彙数が多ければ多いほ
どよい」と考えられてきた．我々が開発してきた形態素解析システムJUMANでは，
当初Wnnかな漢字変換システムの辞書等からスタートしたが，カバレッジが不十
分であるということから一時はEDR辞書を利用していた．しかし，その中には，
多数の複合語，現代では基本的に使用しない古語などが含まれており，形態素解
析に副作用があることや，メンテナンスが困難であるなどの問題があった．
