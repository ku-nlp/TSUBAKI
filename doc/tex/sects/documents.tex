\section{検索対象とする文書集合}
\label{pageset}

TSUBAKIが検索対象としている文書集合は，日本語ページ★100,380,002件である．
これらは，2010年★月から★月にかけて情報通信研究機構 知識処理グループにて
クロールされた約★億★千万件のウェブページ中で，日本語ページとして判定さ
れたものである．クロールは，東京大学田浦研究室にて開発が進められている
★Shim-Crawler\footnote{http://www.logos.t.u-tokyo.ac.jp/crawler/} を利用
して行われた．
%
各ウェブページは，UTF8化，ウェブ標準フォーマット化されており，これらのデータは
API経由で利用可能である（ウェブ標準フォーマットについては第\ref{wsf}章で述べる）．

以下，日本語ウェブページの収集方法について述べる．


\subsection {日本語ウェブページの収集}

日本語ウェブページの抽出処理は以下の手順で行われる．

\begin{description}
\item [Step 1:] Shim-Crawlerの出力をページ単位に分割し，UIDを付与する
\item [Step 2:] 日本語ページ判定処理
\item [Step 3:] ページのutf8化
\end{description}

\noindent
Shim-Crawlerは，クロールした複数のウェブページを単一のファイルに集約し，
gzip圧縮して出力するため，Step 1で，Shim-Crawlerの出力からウェブページを
1件ずつ抽出する．そして，抽出された個々のページに対して，{\bf UID}を付与する．標準フォーマットIDとは10桁の数字と改訂番号からなるIDであ
り、10桁の数字はURLに、改訂番号はURLに適合するページの更新回数に対応する．
標準フォーマットIDは，APIを利用して各ページを取得する際に利用される．
%
続いて，抽出したページが，日本語ウェブページかどうかの判定を行う．クロー
ル結果に日本語以外のページが含まれる場合，Step 1で付与したページID に
欠番が生じるため，日本語ページ判定処理後，再度IDの振りなおしを行う．
%
最後に，ウェブページの文字エンコーディングをutf8 に統一する．

以下，日本語ページ判定処理，utf8変換処理について述べる．


\subsubsection {日本語ページの判定}

文字コードおよび言語情報を手がかりにウェブページが日本語ページかどうかの
判定を行う．\\

\noindent
{\bf (i) 文字コードを用いた日本語ページ判定:}~
ウェブページ中のcharset属性，またはperlの
\texttt{Encode::guess\_encoding()}関数を用いてページの文字コードを調べ
る．文字コードがeuc-jp，x-euc-jp，iso-2022-jp，shift jis，windows-932，
x-sjis，shiftjp，utf-8であれば，そのページを日本語ページの候補とする．\\

\noindent
{\bf (ii) 言語情報を用いた日本語ページ判定:}~
%
(i)で収集されたページのうち，言語情報を用いて日本語を含まないページを排
除する．これは，(i)ではより多くの日本語ページを取得するために，utf8コー
ドでエンコードされているページも日本語ページとして見なしているが，utf8
コードは日本語以外の言語でも用いられており，日本語以外のページが含まれ
ている可能性があるためである．

具体的な処理としては，日本語の助詞（「が」，「を」，「に」，「は」，
「の」，「で」）の含有率が0.5\%以下のページは日本語ページでないと判定
する．

\subsubsection {ウェブページのutf8化}

perlの\texttt{Encode::from\_to()}関数を利用し，ウェブページの文字コード
をutf8に変換する\footnote{変換前の文字コードは，前節(i)の操作により調べ
る．}．この時，ページ内でcharset属性が指定されていても，その値は変更しない．
